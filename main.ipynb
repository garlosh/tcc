{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.semi_supervised import LabelPropagation\n",
    "from nltk.corpus import stopwords\n",
    "from xgboost import XGBClassifier\n",
    "import string\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Carregar o CSV\n",
    "df = pd.read_csv('./dados/pre-processed.csv')\n",
    "\n",
    "# Converter os rótulos para valores numéricos\n",
    "df['label'] = df['label'].apply(lambda x: 1 if x == 'fake' else 0)\n",
    "\n",
    "# Função para limpar as manchetes (remoção de pontuação e stopwords)\n",
    "def limpar_texto(texto):\n",
    "    # Remover pontuações\n",
    "    texto = ''.join([char for char in texto if char not in string.punctuation])\n",
    "    # Converter para minúsculas\n",
    "    texto = texto.lower()\n",
    "    # Remover stopwords\n",
    "    stop_words = set(stopwords.words('portuguese'))\n",
    "    texto = ' '.join([word for word in texto.split() if word not in stop_words])\n",
    "    return texto\n",
    "\n",
    "# Aplicar a função de limpeza nas manchetes\n",
    "df['manchete_limpa'] = df['preprocessed_news'].apply(limpar_texto)\n",
    "\n",
    "# Inicializar o vetorizador TF-IDF\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "# Aplicar o vetorizador nas manchetes limpas\n",
    "X_tfidf = tfidf.fit_transform(df['manchete_limpa'])\n",
    "\n",
    "# Separar a variável preditora (X) e a variável resposta (y)\n",
    "X = X_tfidf  # A matriz TF-IDF é a nossa variável preditora\n",
    "y = df['label']  # Coluna 'label' como variável resposta\n",
    "\n",
    "# Dividir o dataset em treino e teste (80% treino, 20% teste)\n",
    "X_train, X_test, y_train, y_test, idx_train, idx_test = train_test_split(X, y, df.index, test_size=0.2, random_state=42)\n",
    "\n",
    "# Definir diferentes níveis de omissão de rótulos\n",
    "proporcoes = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "\n",
    "# DataFrame para armazenar resultados\n",
    "resultados = []\n",
    "\n",
    "for prop in proporcoes:\n",
    "    # Copiar y_train\n",
    "    y_train_semi = y_train.copy()\n",
    "    \n",
    "    # Determinar quantos rótulos serão removidos\n",
    "    n_remover = int(prop * len(y_train_semi))\n",
    "    indices_remover = np.random.choice(y_train_semi.index, n_remover, replace=False)\n",
    "    \n",
    "    # Remover (ocultar) esses rótulos configurando-os como -1\n",
    "    y_train_semi.loc[indices_remover] = -1\n",
    "\n",
    "    # --- Cenário com LabelPropagation ---\n",
    "    lp = LabelPropagation(kernel='knn')\n",
    "    lp.fit(X_train, y_train_semi)\n",
    "    y_train_propagado = lp.transduction_\n",
    "\n",
    "    # Treinar Logistic Regression com LabelPropagation\n",
    "    modelo_log_lp = LogisticRegression(max_iter=1000)\n",
    "    modelo_log_lp.fit(X_train, y_train_propagado)\n",
    "    y_pred_log_lp = modelo_log_lp.predict(X_test)\n",
    "    acc_log_lp = accuracy_score(y_test, y_pred_log_lp)\n",
    "    report_log_lp = classification_report(y_test, y_pred_log_lp, output_dict=True)\n",
    "\n",
    "    # Treinar XGBoost com LabelPropagation\n",
    "    modelo_xgb_lp = XGBClassifier(eval_metric='logloss')\n",
    "    modelo_xgb_lp.fit(X_train, y_train_propagado)\n",
    "    y_pred_xgb_lp = modelo_xgb_lp.predict(X_test)\n",
    "    acc_xgb_lp = accuracy_score(y_test, y_pred_xgb_lp)\n",
    "    report_xgb_lp = classification_report(y_test, y_pred_xgb_lp, output_dict=True)\n",
    "\n",
    "    # --- Cenário SEM LabelPropagation (descarta as amostras sem rótulo) ---\n",
    "    # Selecionar apenas as amostras que permaneceram rotuladas\n",
    "    mask_labeled = y_train_semi != -1\n",
    "    X_train_no_lp = X_train[mask_labeled]\n",
    "    y_train_no_lp = y_train_semi[mask_labeled]\n",
    "\n",
    "    # Treinar Logistic Regression sem LabelPropagation\n",
    "    modelo_log_no_lp = LogisticRegression(max_iter=1000)\n",
    "    modelo_log_no_lp.fit(X_train_no_lp, y_train_no_lp)\n",
    "    y_pred_log_no_lp = modelo_log_no_lp.predict(X_test)\n",
    "    acc_log_no_lp = accuracy_score(y_test, y_pred_log_no_lp)\n",
    "    report_log_no_lp = classification_report(y_test, y_pred_log_no_lp, output_dict=True)\n",
    "\n",
    "    # Treinar XGBoost sem LabelPropagation\n",
    "    modelo_xgb_no_lp = XGBClassifier(eval_metric='logloss')\n",
    "    modelo_xgb_no_lp.fit(X_train_no_lp, y_train_no_lp)\n",
    "    y_pred_xgb_no_lp = modelo_xgb_no_lp.predict(X_test)\n",
    "    acc_xgb_no_lp = accuracy_score(y_test, y_pred_xgb_no_lp)\n",
    "    report_xgb_no_lp = classification_report(y_test, y_pred_xgb_no_lp, output_dict=True)\n",
    "\n",
    "    # Armazenar resultados desta proporção\n",
    "    resultados.append({\n",
    "        'proporcao_remocao': prop,\n",
    "        # Com LabelPropagation\n",
    "        'acc_log_lp': acc_log_lp,\n",
    "        'precision_log_lp': report_log_lp['weighted avg']['precision'],\n",
    "        'recall_log_lp': report_log_lp['weighted avg']['recall'],\n",
    "        'f1_log_lp': report_log_lp['weighted avg']['f1-score'],\n",
    "        'acc_xgb_lp': acc_xgb_lp,\n",
    "        'precision_xgb_lp': report_xgb_lp['weighted avg']['precision'],\n",
    "        'recall_xgb_lp': report_xgb_lp['weighted avg']['recall'],\n",
    "        'f1_xgb_lp': report_xgb_lp['weighted avg']['f1-score'],\n",
    "        # Sem LabelPropagation\n",
    "        'acc_log_no_lp': acc_log_no_lp,\n",
    "        'precision_log_no_lp': report_log_no_lp['weighted avg']['precision'],\n",
    "        'recall_log_no_lp': report_log_no_lp['weighted avg']['recall'],\n",
    "        'f1_log_no_lp': report_log_no_lp['weighted avg']['f1-score'],\n",
    "        'acc_xgb_no_lp': acc_xgb_no_lp,\n",
    "        'precision_xgb_no_lp': report_xgb_no_lp['weighted avg']['precision'],\n",
    "        'recall_xgb_no_lp': report_xgb_no_lp['weighted avg']['recall'],\n",
    "        'f1_xgb_no_lp': report_xgb_no_lp['weighted avg']['f1-score']\n",
    "    })\n",
    "\n",
    "# Converter resultados para DataFrame\n",
    "df_resultados = pd.DataFrame(resultados)\n",
    "\n",
    "# Salvar os resultados finais em CSV\n",
    "df_resultados.to_csv('comparacao_labelpropagation_vs_sem.csv', index=False)\n",
    "print(\"Resultados salvos em 'comparacao_labelpropagation_vs_sem.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
       "\twith 139054 stored elements and shape (576, 79538)>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_no_lp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
